from pyspark.sql import functions as F
from pyspark.sql import types as T

def flatten_df_with_schema(df):
    """
    Flattens a DataFrame based on its schema. Uses schema inspection to guide the flattening process.
    
    Args:
    df: The input DataFrame.
    
    Returns:
    A flattened DataFrame with no nested fields.
    """
    # Get the initial schema to guide the flattening process
    def extract_schema_fields(schema, prefix=""):
        flat_columns = []  # Holds flat columns (non-nested)
        struct_columns = []  # Holds struct columns to be flattened
        array_columns = []  # Holds array columns to be exploded
        
        for field in schema:
            field_name = field.name
            field_type = field.dataType
            
            if isinstance(field_type, T.StructType):
                # Struct field, needs flattening
                struct_columns.append((f"{prefix}{field_name}", field_type))
            elif isinstance(field_type, T.ArrayType):
                # Array field, needs to be exploded
                array_columns.append((f"{prefix}{field_name}", field_type))
            else:
                # Flat field, keep it as is
                flat_columns.append(f"{prefix}{field_name}")
        
        return flat_columns, struct_columns, array_columns

    def flatten(df, schema, prefix=""):
        flat_cols, struct_cols, array_cols = extract_schema_fields(schema, prefix)
        
        # 1. Select the flat columns (non-struct, non-array)
        df = df.select([F.col(col).alias(col) for col in flat_cols])
        
        # 2. Flatten structs
        for col_name, col_type in struct_cols:
            # Flatten each struct by selecting its subfields
            for field in col_type.fields:
                df = df.withColumn(f"{col_name}_{field.name}", F.col(f"{col_name}.{field.name}"))
            # Drop the original struct column
            df = df.drop(col_name)
        
        # 3. Explode arrays (only for arrays of structs or arrays with > 1 element)
        for col_name, col_type in array_cols:
            if isinstance(col_type.elementType, T.StructType):
                # Explode arrays of structs
                df = df.withColumn(col_name, F.explode_outer(F.col(col_name)))
                # After explosion, recursively flatten the struct inside the array
                df = flatten(df, col_type.elementType, prefix=f"{col_name}_")
        
        # 4. If more nested structs remain, recurse
        nested_fields = [f for f in df.schema.fields if isinstance(f.dataType, (T.StructType, T.ArrayType))]
        if nested_fields:
            df = flatten(df, df.schema)
        
        return df

    # Start the recursive flattening based on schema
    return flatten(df, df.schema)

# Example usage with a JSON DataFrame
# df = spark.read.json("path_to_json_file")
# df_flattened = flatten_df_with_schema(df)
# df_flattened.show(truncate=False)
#################
from pyspark.sql import functions as F
from pyspark.sql import types as T

def flatten_df(df, prefix=""):
    """
    Recursively flattens a nested DataFrame. This version carefully handles arrays to avoid creating fake rows.
    
    Args:
    df: The input DataFrame.
    prefix: A string used to prefix column names, to preserve hierarchy.
    
    Returns:
    A flattened DataFrame with no nested fields.
    """
    flat_cols = []  # Collects all the non-struct/array columns
    nested_cols = []  # Collects all the struct/array columns for flattening
    
    # Loop through all columns in the DataFrame
    for col_name in df.columns:
        col = df[col_name]
        col_type = df.schema[col_name].dataType
        
        # Check if the column is a struct (nested)
        if isinstance(col_type, T.StructType):
            for field in col_type.fields:
                nested_cols.append(F.col(f"{col_name}.{field.name}").alias(f"{prefix}{col_name}_{field.name}"))
                
        # Check if the column is an array, but avoid unnecessary explosion
        elif isinstance(col_type, T.ArrayType):
            # Only explode if the array contains more than one element
            df = df.withColumn(col_name, F.expr(f"filter({col_name}, x -> x is not null)"))  # Clean nulls inside arrays
            if df.select(F.size(F.col(col_name)).alias("size")).filter(F.col("size") > 1).count() > 0:
                # Explode the array only if it contains more than one element
                df = df.withColumn(col_name, F.explode_outer(F.col(col_name)))
                nested_cols.append(F.col(col_name).alias(f"{prefix}{col_name}"))
            else:
                # If the array has 0 or 1 element, treat it as a normal column
                flat_cols.append(F.col(col_name).alias(f"{prefix}{col_name}"))
        
        # Otherwise, it's a flat column, add it directly
        else:
            flat_cols.append(F.col(col_name).alias(f"{prefix}{col_name}"))
           
    # Select flat columns
    df = df.select(flat_cols + nested_cols)
    
    # If there are nested fields, recurse on the DataFrame
    for nested_col in nested_cols:
        # Check if there are still nested fields in the DataFrame
        nested_fields = [f for f in df.schema.fields if isinstance(f.dataType, (T.StructType, T.ArrayType))]
        if nested_fields:
            # Recursively flatten again if nested fields exist
            df = flatten_df(df, prefix=f"{prefix}")
    
    return df

# Example usage with your JSON data loaded into a DataFrame:
# df = spark.read.json("path_to_json_file")
# df_flattened = flatten_df(df)
# df_flattened.show(truncate=False)






####################################â„–

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
import json

# Sample data
data = [
    {
        "carrier": {
            "iata": "KE",
            "icao": "KAL"
        },
        "flightNumber": 476,
        "departure": {
            "airport": {
                "iata": "SGN",
                "icao": "VVTS"
            },
            "terminal": "2",
            "date": {
                "local": "2023-10-03",
                "utc": "2023-10-02"
            },
            "time": {
                "local": "00:15",
                "utc": "17:15"
            }
        },
        "arrival": {
            "airport": {
                "iata": "ICN",
                "icao": "RKSI"
            },
            "terminal": "2",
            "date": {
                "local": "2023-10-03",
                "utc": "2023-10-02"
            },
            "time": {
                "local": "07:20",
                "utc": "22:20"
            }
        },
        "elapsedTime": 305,
        "codeshare": {
            "cockpitCrewEmployer": {
                "code": "KE",
                "name": "",
                "number": "4"
            },
            "marketingFlights": [
                {"code": "DL", "serviceNumber": "7926"},
                {"code": "VN", "serviceNumber": "3400"},
                {"code": "VS", "serviceNumber": "5520"}
            ]
        }
    }
    # Add more records as necessary
]

# Initialize Spark
spark = SparkSession.builder.master("local[*]").appName("Flatten JSON").getOrCreate()

# Convert the list of dictionaries into a DataFrame
df = spark.read.json(spark.sparkContext.parallelize([json.dumps(data)]))

# Function to flatten nested structure
def flatten_df(df, prefix=None):
    # Loop over columns, checking for structs (nested fields) or arrays
    for col_name in df.columns:
        col = df[col_name]
        if isinstance(df.schema[col_name].dataType, T.StructType):
            # If it's a Struct, expand it
            df = df.select("*", F.col(f"{col_name}.*"))
            df = df.drop(col_name)
        elif isinstance(df.schema[col_name].dataType, T.ArrayType):
            # If it's an Array, explode it
            df = df.withColumn(col_name, F.explode_outer(col))
    return df

# Call the flatten function
flat_df = flatten_df(df)

# Show the result
flat_df.show(truncate=False)
#####################################################################
def load_data(url):

    # 0 Extract the file name from the URL
    parsed_url = urlparse(url)
    file_name = os.path.basename(parsed_url.path)
    
    # 1 Load data from URL
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        print(f"#1# {file_name} data successfully loaded #1#")
    else:
        print(f"#1# Failed to load {file_name}. Status code: {response.status_code} #1#")
        return None  
    
    # 2 Determine the structure of the data and find keys
    if isinstance(data, dict):
    
    # 2.0.1 Look for keys in the dictionary
        keys = list(data.keys())
        print(f"#2.0.1#Top-level keys found: {keys}#2.0.1#")
        
    # 2.0.2 If there's a key that is a list, we can sample from it
        for key in keys:
            if isinstance(data[key], list):
                data_sample = data[key][:1]  # NOTE: Get entry
                for entry in data_sample:
                    print(f"#2.0.2# Raw Data from key: '{key}'#2.0.2#")
                    print(f"#2.0.2# Raw Data sample: {json.dumps(entry, indent=4)} #2.0.2#")
                break
        else:
            print("#2.0.2# No list-type keys found in the dictionary.#2.0.2#")
    
    elif isinstance(data, list):
    
    #2.0.3 Print sample directly from the list
        data_sample = data[:1]  # NOTE: Get entry
        for entry in data_sample:
            print(f"#2.0.3# Raw Data: sample: {json.dumps(entry, indent=4)} #2.0.3#")
    
    else:
        print("#2.0.3# No valid data found. #2.0.3#")

    # 2.1 Create a temporary file with a name based on the URL 
    tmp_file_path = os.path.join(tempfile.gettempdir(), file_name)
    urllib.request.urlretrieve(url, tmp_file_path)

    # 3 Check temporary file
    if os.path.exists(tmp_file_path):
        print(f"#3# Temporary file {file_name} exists #3#")
    else:
        print(f"#3# Temporary file {file_name} does not exist #3#")
        return None
    
    # 4 Validate the JSON structure
    try:
        with open(tmp_file_path, "r") as f:
            data = json.load(f)
            print(f"#4# {file_name} is valid #4#")
    except json.JSONDecodeError as e:
        print(f"#4# Invalid {file_name} structure: #4#")
        print(f"#4# {str(e)} #4#") # NOTE:exeption message
        return None

    # 5 Load into Spark DataFrame with error handling
    try:
        df = spark.read.json(tmp_file_path)
        df.show(1)
    
    except Exception as w:
        print("#5# Error loading DataFrame in single line option: #5#")
        print(f"#5# {str(w)} #5#") # NOTE:exeption message
        print("#5# Attempting to load DataFrame in multi line option. #5#")
        
        try:
            df = spark.read.option("multiline", "true").json(tmp_file_path)
            print("#5# DataFrame loaded successfully in multiline option #5#")
        except Exception as q:
            print("#5# Error loading DataFrame in multiline option: #5#")
            print(f"#5# {str(q)} #5#") # NOTE:exeption message
            return None
    

    # 6 Show the DataFrame
    print("#6# DataFrame 1st line #6#")
    df.show(1)
    return df
##########################################
import os
import json
import requests
import tempfile
import urllib
from urllib.parse import urlparse
from pyspark.sql import SparkSession

# Initialize Spark
spark = SparkSession.builder.master("local[*]").appName("DataLoader").getOrCreate()

def load_data(url):
    # 0 Extract the file name from the URL
    parsed_url = urlparse(url)
    file_name = os.path.basename(parsed_url.path)

    # 1 Load data from URL
    data = download_json_data(url)
    if data is None:
        return None  # Exit if download failed
    
    # 2 Determine the structure of the data and find keys
    analyze_json_structure(data)

    # 2.1 Save the file to a temporary location
    tmp_file_path = save_to_temp_file(url, file_name)
    if tmp_file_path is None:
        return None  # Exit if file creation failed
    
    # 4 Validate the JSON structure
    if not validate_json_structure(tmp_file_path):
        return None  # Exit if validation failed
    
    # 5 Load into Spark DataFrame with error handling
    df = load_spark_dataframe(tmp_file_path)
    if df is None:
        return None  # Exit if DataFrame loading failed

    # 6 Show the DataFrame
    print("#6# DataFrame 1st line #6#")
    df.show(1)
    return df

def download_json_data(url):
    """
    Downloads the JSON data from the given URL.
    """
    response = requests.get(url)
    file_name = os.path.basename(urlparse(url).path)

    if response.status_code == 200:
        print(f"#1# {file_name} data successfully loaded #1#")
        return response.json()
    else:
        print(f"#1# Failed to load {file_name}. Status code: {response.status_code} #1#")
        return None

def analyze_json_structure(data):
    """
    Analyze the structure of the JSON data and print a sample.
    """
    if isinstance(data, dict):
        # 2.0.1 Look for keys in the dictionary
        keys = list(data.keys())
        print(f"#2.0.1# Top-level keys found: {keys} #2.0.1#")
        
        # 2.0.2 If there's a key that is a list, we can sample from it
        for key in keys:
            if isinstance(data[key], list):
                data_sample = data[key][:1]  # Get the first entry
                for entry in data_sample:
                    print(f"#2.0.2# Raw Data from key: '{key}' #2.0.2#")
                    print(f"#2.0.2# Raw Data sample: {json.dumps(entry, indent=4)} #2.0.2#")
                break
        else:
            print("#2.0.2# No list-type keys found in the dictionary. #2.0.2#")
    
    elif isinstance(data, list):
        # 2.0.3 Print sample directly from the list
        data_sample = data[:1]  # Get the first entry
        for entry in data_sample:
            print(f"#2.0.3# Raw Data sample: {json.dumps(entry, indent=4)} #2.0.3#")
    else:
        print("#2.0.3# No valid data found. #2.0.3#")

def save_to_temp_file(url, file_name):
    """
    Save the content from the URL to a temporary file and return the path.
    """
    tmp_file_path = os.path.join(tempfile.gettempdir(), file_name)
    
    try:
        urllib.request.urlretrieve(url, tmp_file_path)
        if os.path.exists(tmp_file_path):
            print(f"#3# Temporary file {file_name} exists #3#")
            return tmp_file_path
        else:
            print(f"#3# Temporary file {file_name} does not exist #3#")
            return None
    except Exception as e:
        print(f"#3# Error saving file {file_name}: {str(e)} #3#")
        return None

def validate_json_structure(file_path):
    """
    Validate the structure of the JSON file.
    """
    try:
        with open(file_path, "r") as f:
            json.load(f)
        print(f"#4# {os.path.basename(file_path)} is valid #4#")
        return True
    except json.JSONDecodeError as e:
        print(f"#4# Invalid JSON structure: {str(e)} #4#")
        return False

def load_spark_dataframe(file_path):
    """
    Load the JSON data into a Spark DataFrame with error handling.
    """
    try:
        df = spark.read.json(file_path)
        print("#5# DataFrame loaded successfully (single line option) #5#")
        return df
    except Exception as e:
        print(f"#5# Error loading DataFrame in single line option: {str(e)} #5#")
        print("#5# Attempting to load DataFrame with multiline option. #5#")
        
        try:
            df = spark.read.option("multiline", "true").json(file_path)
            print("#5# DataFrame loaded successfully in multiline option #5#")
            return df
        except Exception as e:
            print(f"#5# Error loading DataFrame in multiline option: {str(e)} #5#")
            return None
###################################
import requests
import os
import json
import tempfile
from io import StringIO
from urllib.parse import urlparse
from pyspark.sql import SparkSession

# Initialize Spark
spark = SparkSession.builder.master("local[*]").appName("OptimizedDataLoader").getOrCreate()

def load_data(url):
    """
    Optimized function to download data from URL and load it into a Spark DataFrame.
    """
    # 0 Extract the file name from the URL
    file_name = os.path.basename(urlparse(url).path)
    
    # 1 Download the data
    data = stream_download(url)
    if data is None:
        return None  # Exit if download failed

    # 2 Analyze the structure (optional if the JSON is large and directly handled by Spark)
    analyze_json_structure(data)
    
    # 3 Try loading into Spark without saving to disk
    try:
        df = load_spark_dataframe_from_memory(data)
        print("#5# DataFrame loaded successfully from memory #5#")
    except Exception as e:
        print(f"#5# Error loading DataFrame from memory: {str(e)} #5#")
        print("#5# Attempting to load DataFrame using file-based approach #5#")

        # Fallback: Save to a temporary file and read into Spark
        tmp_file_path = save_to_temp_file(data, file_name)
        if tmp_file_path is None:
            return None

        # Validate and load the DataFrame from the file
        df = load_spark_dataframe(tmp_file_path)
        if df is None:
            return None  # Exit if DataFrame loading failed
    
    # 6 Show the DataFrame
    print("#6# DataFrame 1st line #6#")
    df.show(1)
    return df

def stream_download(url):
    """
    Downloads the data from the URL and returns it as a string (JSON) for further processing.
    """
    try:
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            print(f"#1# Data from {url} successfully loaded #1#")
            return response.text  # Return the raw JSON data as string
        else:
            print(f"#1# Failed to load {url}. Status code: {response.status_code} #1#")
            return None
    except requests.RequestException as e:
        print(f"#1# Error while downloading data from {url}: {str(e)} #1#")
        return None

def analyze_json_structure(data):
    """
    Analyzes the structure of the JSON data (string) and prints a sample.
    """
    try:
        json_data = json.loads(data)
        if isinstance(json_data, dict):
            keys = list(json_data.keys())
            print(f"#2.0.1# Top-level keys found: {keys} #2.0.1#")
        elif isinstance(json_data, list):
            print(f"#2.0.3# Sample data from the list: {json.dumps(json_data[:1], indent=4)} #2.0.3#")
        else:
            print("#2.0.3# No valid data structure found. #2.0.3#")
    except json.JSONDecodeError:
        print("#2.0.3# Failed to parse JSON structure. Skipping analysis. #2.0.3#")

def save_to_temp_file(data, file_name):
    """
    Saves the JSON string to a temporary file and returns the file path.
    """
    try:
        tmp_file_path = os.path.join(tempfile.gettempdir(), file_name)
        with open(tmp_file_path, "w") as f:
            f.write(data)
        print(f"#3# Temporary file {file_name} saved #3#")
        return tmp_file_path
    except Exception as e:
        print(f"#3# Error saving temporary file {file_name}: {str(e)} #3#")
        return None

def load_spark_dataframe_from_memory(data):
    """
    Loads the JSON data into a Spark DataFrame from an in-memory string.
    """
    try:
        # Use StringIO to convert string to a file-like object
        json_rdd = spark.sparkContext.parallelize([data])
        df = spark.read.json(json_rdd)
        return df
    except Exception as e:
        print(f"#5# Error loading DataFrame from memory: {str(e)} #5#")
        raise e  # Re-raise the exception for the calling function to handle

def load_spark_dataframe(file_path):
    """
    Loads the JSON data from a file into a Spark DataFrame with error handling.
    """
    try:
        df = spark.read.json(file_path)
        print("#5# DataFrame loaded successfully from file #5#")
        return df
    except Exception as e:
        print(f"#5# Error loading DataFrame from file: {str(e)} #5#")
        print("#5# Attempting to load DataFrame using multiline option #5#")
        
        try:
            df = spark.read.option("multiline", "true").json(file_path)
            print("#5# DataFrame loaded successfully in multiline option #5#")
            return df
        except Exception as e:
            print(f"#5# Error loading DataFrame in multiline option: {str(e)} #5#")
            return None
from pyspark.sql import functions as F
from pyspark.sql import types as T

def flatten_df(df, prefix=""):
    """
    Recursively flattens a DataFrame. Handles structs (nested fields) and arrays (by exploding them).
    
    Args:
    df: The input DataFrame.
    prefix: Used to prefix column names to maintain the hierarchy in the flattened structure.
    
    Returns:
    A flattened DataFrame with no nested fields.
    """
    flat_cols = []  # Collects all the non-struct/array columns
    nested_cols = []  # Collects all the struct/array columns for flattening
    
    # Loop through all columns in the DataFrame
    for col_name in df.columns:
        col = df[col_name]
        col_type = df.schema[col_name].dataType
        
        # Check if the column is a struct (nested)
        if isinstance(col_type, T.StructType):
            for field in col_type.fields:
                nested_cols.append(F.col(f"{col_name}.{field.name}").alias(f"{prefix}{col_name}_{field.name}"))
        
        # Check if the column is an array, then explode it
        elif isinstance(col_type, T.ArrayType):
            # Explode the array while maintaining the prefix for hierarchical naming
            df = df.withColumn(col_name, F.explode_outer(F.col(col_name)))
            nested_cols.append(F.col(col_name).alias(f"{prefix}{col_name}"))
        
        # Otherwise, it's a flat column, add it directly
        else:
            flat_cols.append(F.col(col_name).alias(f"{prefix}{col_name}"))
    
    # Select flat columns
    df = df.select(flat_cols + nested_cols)
    
    # If there are nested fields, recurse on the DataFrame
    for nested_col in nested_cols:
        # Check if there are still nested fields in the DataFrame
        nested_fields = [f for f in df.schema.fields if isinstance(f.dataType, (T.StructType, T.ArrayType))]
        if nested_fields:
            # Recursively flatten again if nested fields exist
            df = flatten_df(df, prefix=f"{prefix}")
    
    return df
########################################
# Example usage:
# df is your original dataframe
flat_df = flatten_df(df)
flat_df.show(truncate=False)
#########################################

from pyspark.sql import functions as F
from pyspark.sql.types import StringType, DoubleType

def clean_data(df):
    """
    Cleans a DataFrame by handling missing values, fixing data types, and removing constant columns.
    
    Args:
    df: The input Spark DataFrame.
    
    Returns:
    A cleaned DataFrame.
    """
    # 1. Handle Missing Values (NaN, empty strings, zeros)
    df = handle_missing_values(df)
    
    # 2. Fix Mixed Data Types (e.g., strings and numbers in the same column)
    df = fix_mixed_data_types(df)
    
    # 3. Drop Columns with Constant Values
    df = drop_constant_columns(df)
    
    return df

def handle_missing_values(df):
    """
    Replaces NaN, empty strings, and zeros with None (null).
    
    Args:
    df: The input Spark DataFrame.
    
    Returns:
    DataFrame with missing values handled.
    """
    # Replace empty strings and 0s with None
    df = df.replace("", None).replace(0, None)
    
    # Optionally, you can drop rows with null values (or use fillna() to replace them with default values)
    df = df.na.drop()  # Drops rows with any null values
    
    return df

def fix_mixed_data_types(df):
    """
    Detects and fixes columns with mixed data types (e.g., numbers as strings) in a DataFrame.
    
    Args:
    df: The input Spark DataFrame.
    
    Returns:
    DataFrame with consistent data types.
    """
    for col in df.columns:
        # Detect if the column has mixed types (e.g., strings and numbers)
        if df.select(col).dtypes[0][1] == 'string':
            # Attempt to convert strings to numbers where applicable
            df = df.withColumn(col, F.when(F.col(col).cast(DoubleType()).isNotNull(), F.col(col).cast(DoubleType())).otherwise(F.col(col)))
    return df

def drop_constant_columns(df):
    """
    Detects and removes columns with constant values (i.e., all values are the same).
    
    Args:
    df: The input Spark DataFrame.
    
    Returns:
    DataFrame without constant-value columns.
    """
    for col in df.columns:
        distinct_count = df.select(F.countDistinct(col)).collect()[0][0]
        if distinct_count == 1:
            df = df.drop(col)
            print(f"Column '{col}' has constant values and was dropped.")
    
    return df

def transform_string_case(df, to_upper=True):
    """
    Transforms all string columns to either uppercase or lowercase.
    
    Args:
    df: The input Spark DataFrame.
    to_upper: If True, converts strings to uppercase, otherwise to lowercase.
    
    Returns:
    DataFrame with transformed string columns.
    """
    for col, dtype in df.dtypes:
        if dtype == 'string':
            if to_upper:
                df = df.withColumn(col, F.upper(F.col(col)))
            else:
                df = df.withColumn(col, F.lower(F.col(col)))
    return df

def report_data_issues(df):
    """
    Prints a report of data issues: mixed types, columns with nulls, and constant-value columns.
    
    Args:
    df: The input Spark DataFrame.
    """
    print("# Data Issues Report #")
    
    # 1. Mixed types detection
    print("Columns with mixed types:")
    for col in df.columns:
        if df.select(col).dtypes[0][1] == 'string':
            mixed_type_count = df.select(col).filter(F.col(col).cast(DoubleType()).isNotNull()).count()
            if mixed_type_count > 0:
                print(f" - {col} contains mixed strings and numbers.")
    
    # 2. Columns with nulls
    print("\nColumns with null values:")
    for col in df.columns:
        null_count = df.filter(F.col(col).isNull()).count()
        if null_count > 0:
            print(f" - {col} has {null_count} null values.")
    
    # 3. Constant-value columns
    print("\nColumns with constant values:")
    for col in df.columns:
        distinct_count = df.select(F.countDistinct(col)).collect()[0][0]
        if distinct_count == 1:
            print(f" - {col} contains only one unique value.")
#######################################

from pyspark.sql import functions as F
from pyspark.sql.types import StringType, DoubleType, IntegerType

# Reusing previously created functions for cleaning, etc.

def clean_data(df):
    df = handle_missing_values(df)
    df = fix_mixed_data_types(df)
    df = drop_constant_columns(df)
    return df

def handle_missing_values(df):
    df = df.replace("", None).replace(0, None)
    df = df.na.drop()  # Optional: Drop rows with any null values
    return df

def fix_mixed_data_types(df):
    for col in df.columns:
        if df.select(col).dtypes[0][1] == 'string':
            df = df.withColumn(col, F.when(F.col(col).cast(DoubleType()).isNotNull(), F.col(col).cast(DoubleType())).otherwise(F.col(col)))
    return df

def drop_constant_columns(df):
    for col in df.columns:
        distinct_count = df.select(F.countDistinct(col)).collect()[0][0]
        if distinct_count == 1:
            df = df.drop(col)
            print(f"Column '{col}' has constant values and was dropped.")
    return df

def transform_string_case(df, to_upper=True):
    for col, dtype in df.dtypes:
        if dtype == 'string':
            if to_upper:
                df = df.withColumn(col, F.upper(F.col(col)))
            else:
                df = df.withColumn(col, F.lower(F.col(col)))
    return df

def report_data_issues(df):
    print("# Data Issues Report #")
    
    for col in df.columns:
        mixed_type_count = df.select(col).filter(F.col(col).cast(DoubleType()).isNotNull()).count()
        if mixed_type_count > 0:
            print(f" - {col} contains mixed strings and numbers.")
    
    for col in df.columns:
        null_count = df.filter(F.col(col).isNull()).count()
        if null_count > 0:
            print(f" - {col} has {null_count} null values.")
    
    for col in df.columns:
        distinct_count = df.select(F.countDistinct(col)).collect()[0][0]
        if distinct_count == 1:
            print(f" - {col} contains only one unique value.")

def identify_column_data_types(df):
    print("# Identified Data Types for Columns #")
    column_types = {}
    for col, dtype in df.dtypes:
        print(f"Column '{col}' has data type: {dtype}")
        column_types[col] = dtype
    return column_types

def identify_mixed_data_patterns(df):
    print("# Detecting Mixed Data Patterns in Columns #")
    mixed_columns = {}
    for col in df.columns:
        print(f"Analyzing column '{col}' for mixed data patterns...")
        df = df.withColumn(f"{col}_type", 
                           F.when(F.col(col).cast(DoubleType()).isNotNull(), "number")
                           .when(F.col(col).cast(IntegerType()).isNotNull(), "integer")
                           .when(F.col(col).cast(StringType()).isNotNull(), "string")
                           .otherwise("unknown"))
        
        distinct_types = df.select(f"{col}_type").distinct().rdd.flatMap(lambda x: x).collect()
        
        if len(distinct_types) > 1:
            print(f"Column '{col}' has mixed data types: {distinct_types}")
            mixed_columns[col] = distinct_types
        else:
            print(f"Column '{col}' has consistent data type: {distinct_types[0]}")
        
        df = df.drop(f"{col}_type")
    
    return mixed_columns

# Example usage:

# Assume df is your Spark DataFrame
cleaned_df = clean_data(df)

# Identify column data types
column_data_types = identify_column_data_types(cleaned_df)

# Detect mixed data patterns in columns
mixed_data_patterns = identify_mixed_data_patterns(cleaned_df)

# Report data issues like mixed types, nulls, and constant-value columns
report_data_issues(cleaned_df)
###########################################

from pyspark.sql import functions as F

def calculate_total_delay_per_airport(df):
    """
    Calculate the total delay in minutes for both departures and arrivals by airport.
    
    Args:
    df: The input DataFrame containing flight data.
    
    Returns:
    Two DataFrames:
    - Total departure delay per airport.
    - Total arrival delay per airport.
    """
    
    # 1. Extract relevant fields for departure delays (outGateVariation) and airport
    df_departure_delays = df.select(
        F.col("departure.airport.iata").alias("departure_airport"),
        F.col("statusDetails.departure.actualTime.outGateVariation").alias("departure_delay")
    )
    
    # 2. Extract relevant fields for arrival delays (inGateVariation) and airport
    df_arrival_delays = df.select(
        F.col("arrival.airport.iata").alias("arrival_airport"),
        F.col("statusDetails.arrival.actualTime.inGateVariation").alias("arrival_delay")
    )
    
    # 3. Convert delay fields to minutes (if the delay is in the format "HH:MM:SS")
    df_departure_delays = df_departure_delays.withColumn(
        "departure_delay_minutes", 
        F.expr("substring(departure_delay, 4, 2)").cast("int") * 60 + F.expr("substring(departure_delay, 7, 2)").cast("int")
    )
    
    df_arrival_delays = df_arrival_delays.withColumn(
        "arrival_delay_minutes", 
        F.expr("substring(arrival_delay, 4, 2)").cast("int") * 60 + F.expr("substring(arrival_delay, 7, 2)").cast("int")
    )
    
    # 4. Group by airport and sum the delays
    total_departure_delay = df_departure_delays.groupBy("departure_airport").agg(
        F.sum("departure_delay_minutes").alias("total_departure_delay")
    )
    
    total_arrival_delay = df_arrival_delays.groupBy("arrival_airport").agg(
        F.sum("arrival_delay_minutes").alias("total_arrival_delay")
    )
    
    return total_departure_delay, total_arrival_delay

# Assume df is the DataFrame created from omg.json
total_departure_delay, total_arrival_delay = calculate_total_delay_per_airport(df)

# Show the results
total_departure_delay.show(truncate=False)
total_arrival_delay.show(truncate=False)
########################################
import pyspark.sql.functions as F
from pyspark.sql.types import StringType, LongType

def identify_time_columns(df):
    """
    Identifies columns with time-related data: UNIX timestamp columns and time-like (HH:MM:SS) columns.
    
    Args:
    df: The input Spark DataFrame.
    
    Returns:
    A dictionary with two keys: 
    - 'unix_columns' for columns with UNIX timestamps,
    - 'time_format_columns' for columns with time-like formats (HH:MM:SS).
    """
    unix_timestamp_cols = []
    time_format_cols = []
    
    # 1. Identify UNIX timestamp columns (usually numeric fields in seconds since 1970)
    for col, dtype in df.dtypes:
        if dtype in ['long', 'int']:
            # Check if the numeric value falls into the plausible range of UNIX timestamps
            min_value = df.select(F.min(col)).collect()[0][0]
            max_value = df.select(F.max(col)).collect()[0][0]
            
            # UNIX timestamps should be within a certain range, e.g., between 1970 and a plausible year
            if min_value and max_value and (min_value > 0 and max_value > 1_000_000_000):  # After 1970
                unix_timestamp_cols.append(col)
    
    # 2. Identify time format columns (e.g., HH:MM:SS format)
    for col, dtype in df.dtypes:
        if dtype == 'string':
            # Check if the column contains values in the format HH:MM:SS
            sample_value = df.select(col).where(F.col(col).isNotNull()).limit(1).collect()
            if sample_value and len(sample_value) > 0 and ":" in sample_value[0][0]:
                time_format_cols.append(col)
    
    print(f"Identified UNIX timestamp columns: {unix_timestamp_cols}")
    print(f"Identified time format columns (e.g., HH:MM:SS): {time_format_cols}")
    
    return {
        'unix_columns': unix_timestamp_cols,
        'time_format_columns': time_format_cols
    }

def convert_unix_time_columns(df, unix_columns):
    """
    Converts columns with UNIX timestamps into a human-readable format (yyyy-MM-dd HH:mm:ss).
    
    Args:
    df: The input Spark DataFrame.
    unix_columns: A list of columns that store UNIX timestamps.
    
    Returns:
    A DataFrame with the UNIX timestamp columns converted to human-readable format.
    """
    for col in unix_columns:
        print(f"Converting column '{col}' from UNIX timestamp to human-readable format.")
        df = df.withColumn(col, F.from_unixtime(F.col(col), "yyyy-MM-dd HH:mm:ss"))
    return df

def fix_time_format_columns(df, time_format_columns):
    """
    Fixes inconsistencies in columns with time-like data (e.g., HH:MM:SS format).
    This is a placeholder function for now, which can be extended if needed.
    
    Args:
    df: The input Spark DataFrame.
    time_format_columns: A list of columns that store time in formats like HH:MM:SS.
    
    Returns:
    The DataFrame, possibly with standardized time formats.
    """
    for col in time_format_columns:
        print(f"Column '{col}' is already in time format (e.g., HH:MM:SS).")
        # Currently, we're not converting HH:MM:SS format. If required, add more logic here.
    return df

# Example workflow combining the three functions:

# Step 1: Identify time-related columns
time_columns_info = identify_time_columns(df)

# Step 2: Convert UNIX timestamp columns to human-readable format
df = convert_unix_time_columns(df, time_columns_info['unix_columns'])

# Step 3: Fix time format columns if necessary (e.g., handle mixed formats)
df = fix_time_format_columns(df, time_columns_info['time_format_columns'])

# Show the updated DataFrame
df.show(truncate=False)
##############################
def count_delayed_departures(df):
    """
    Counts the number of delayed departures.
    
    Args:
    df: The input flattened Spark DataFrame.
    
    Returns:
    A DataFrame showing the count of delayed departures per airport.
    """
    # Check if the departure delay is non-zero and count those
    delayed_departures = df.filter(F.col("departure_actualTime_outGateVariation").isNotNull()) \
                           .filter(F.col("departure_actualTime_outGateVariation") > 0) \
                           .groupBy("departure_airport_iata") \
                           .agg(F.count("*").alias("number_of_delayed_departures"))
    
    return delayed_departures
#######################
def total_departure_delay_minutes(df):
    """
    Calculates the total minutes of delay for departures.
    
    Args:
    df: The input flattened Spark DataFrame.
    
    Returns:
    A DataFrame showing the total delay in minutes for departures per airport.
    """
    # Extract delay duration in HH:MM:SS format and convert it to minutes
    departure_delay = df.filter(F.col("departure_actualTime_outGateVariation").isNotNull()) \
                        .withColumn("departure_delay_minutes", 
                                    F.expr("substring(departure_actualTime_outGateVariation, 4, 2)").cast("int") * 60 + 
                                    F.expr("substring(departure_actualTime_outGateVariation, 7, 2)").cast("int")) \
                        .groupBy("departure_airport_iata") \
                        .agg(F.sum("departure_delay_minutes").alias("total_departure_delay_minutes"))
    
    return departure_delay
#########################
def count_delayed_arrivals(df):
    """
    Counts the number of delayed arrivals.
    
    Args:
    df: The input flattened Spark DataFrame.
    
    Returns:
    A DataFrame showing the count of delayed arrivals per airport.
    """
    # Check if the arrival delay is non-zero and count those
    delayed_arrivals = df.filter(F.col("arrival_actualTime_inGateVariation").isNotNull()) \
                         .filter(F.col("arrival_actualTime_inGateVariation") > 0) \
                         .groupBy("arrival_airport_iata") \
                         .agg(F.count("*").alias("number_of_delayed_arrivals"))
    
    return delayed_arrivals
############################
def total_arrival_delay_minutes(df):
    """
    Calculates the total minutes of delay for arrivals.
    
    Args:
    df: The input flattened Spark DataFrame.
    
    Returns:
    A DataFrame showing the total delay in minutes for arrivals per airport.
    """
    # Extract delay duration in HH:MM:SS format and convert it to minutes
    arrival_delay = df.filter(F.col("arrival_actualTime_inGateVariation").isNotNull()) \
                      .withColumn("arrival_delay_minutes", 
                                  F.expr("substring(arrival_actualTime_inGateVariation, 4, 2)").cast("int") * 60 + 
                                  F.expr("substring(arrival_actualTime_inGateVariation, 7, 2)").cast("int")) \
                      .groupBy("arrival_airport_iata") \
                      .agg(F.sum("arrival_delay_minutes").alias("total_arrival_delay_minutes"))
    
    return arrival_delay
