{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, avg, count\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Importing Python Data Stat. libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For error handling and utilities \n",
    "import json\n",
    "import requests\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# TODO: 1 check warnings \n",
    "# TODO: 2 add corupted json handling in to function or make corupted json function \n",
    "# TODO: 3 re-write reading json via Spark as function, double check logic.for controls\n",
    "# TODO: 4 harmonize error messages (numbers and explanation for readme) and code descriptions\n",
    "# TODO: 5 Verification of data \n",
    "# TODO: 6 Handling of missing values \n",
    "\n",
    "# NOTE: 1 reading more complex file might be necessary to pre process json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 22:57:49 WARN Utils: Your hostname, codespaces-358acb resolves to a loopback address: 127.0.0.1; using 10.0.2.21 instead (on interface eth0)\n",
      "24/10/11 22:57:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/11 22:57:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark builder (check warnings - port 4041)\n",
    "spark = SparkSession.builder.appName(\"AirportFlightDataViz\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsb.json data successfully loaded\n",
      "data_adsb {'AircraftId': '400960', 'Latitude': 10.81889, 'Longitude': 106.65194, 'Track': 30, 'Altitude': 0, 'Speed': 0, 'Squawk': 7713, 'Type': 'A320', 'Registration': 'G-TTOE', 'LastUpdate': 1696278420, 'Origin': 'SGN', 'Destination': 'ICN', 'Flight': 'BA484', 'Onground': 1, 'Vspeed': 0, 'Callsign': 'BAW476C', 'SourceType': 'ADS-B FR24 receivers', 'ETA': 0}\n",
      "tmp file_absd.json exist\n",
      "absd.json is valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading DataFrame in single line option\n",
      "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
      "referenced columns only include the internal corrupt record column\n",
      "(named _corrupt_record by default). For example:\n",
      "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
      "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
      "Instead, you can cache or save the parsed results and then send the same query.\n",
      "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
      "df.filter($\"_corrupt_record\".isNotNull).count().\n",
      "Attempt to load DataFrame in multi line option\n",
      "Dataframe Loaded succesfully\n",
      "+----------+--------+--------+-----------+---+------+----------+--------+---------+--------+------+-------+------------+--------------------+-----+------+-----+----+------+\n",
      "|AircraftId|Altitude|Callsign|Destination|ETA|Flight|LastUpdate|Latitude|Longitude|Onground|Origin|RadarId|Registration|          SourceType|Speed|Squawk|Track|Type|Vspeed|\n",
      "+----------+--------+--------+-----------+---+------+----------+--------+---------+--------+------+-------+------------+--------------------+-----+------+-----+----+------+\n",
      "|    400960|       0| BAW476C|        ICN|  0| BA484|1696278420|10.81889|106.65194|       1|   SGN|   NULL|      G-TTOE|ADS-B FR24 receivers|    0|  7713|   30|A320|     0|\n",
      "+----------+--------+--------+-----------+---+------+----------+--------+---------+--------+------+-------+------------+--------------------+-----+------+-----+----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 22:58:08 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "url_adsb = \"https://raw.githubusercontent.com/JM-AE/Airport-Flight-Data/refs/heads/main/adsb.json\"\n",
    "response_adsb = requests.get(url_adsb)\n",
    "\n",
    "# Check if file is possible to load using py.json\n",
    "if response_adsb.status_code == 200:\n",
    "    data_adsb = response_adsb.json()\n",
    "    print(\"adsb.json data successfully loaded\")\n",
    "else: print(f\"Failed to load adsb.json Status code:{response_adsb.status_code}\")\n",
    "\n",
    "# Print sample of .json file\n",
    "for entry_adsb in data_adsb[:1]:\n",
    "    print(\"data_adsb\",entry_adsb)\n",
    "\n",
    "# Request save as tmp file \n",
    "urllib.request.urlretrieve(url_adsb,\"/tmp/file_absd.json\")\n",
    "\n",
    "# Check if file exist on correct address\n",
    "if os.path.exists(\"/tmp/file_absd.json\"):\n",
    "    print(\"tmp file_absd.json exist\")\n",
    "else: \n",
    "    print(\"tmp file_absd.json do not exist\")\n",
    "\n",
    "# Check if .joson valid or not\n",
    "with open(\"/tmp/file_absd.json\",\"r\") as f:\n",
    "    try:\n",
    "       data_adsb = json.load(f)\n",
    "       print(\"absd.json is valid\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid absd.json structure: {e}\") \n",
    "\n",
    "# Load in to Spark DF with check for error     \n",
    "try:\n",
    "    df_absd =spark.read.json(\"/tmp/file_absd.json\")\n",
    "    df_absd.show(3)\n",
    "except Exception as t:\n",
    "    print(f\"Error loading DataFrame in single line option\")\n",
    "    print(str(t))\n",
    "    print(f\"Attempt to load DataFrame in multi line option\")\n",
    "    try: \n",
    "        df_absd =spark.read.option(\"multiline\", \"true\").json(\"/tmp/file_absd.json\")\n",
    "        print(f\"Dataframe Loaded succesfully\")\n",
    "    except Exception as w:\n",
    "        print(f\"Error loading DataFrame in multi line option:{str(w)}\")\n",
    "\n",
    "df_absd.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|Origin|AverageSpeed|\n",
      "+------+------------+\n",
      "|GUA   |170.0       |\n",
      "|DOH   |250.0       |\n",
      "|SGN   |234.2       |\n",
      "|IAD   |170.0       |\n",
      "+------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_speed_per_airport_origin = df_absd.groupBy(\"Origin\").agg(F.avg(\"Speed\").alias(\"AverageSpeed\"))\n",
    "avg_speed_per_airport_origin.show(truncate=False)\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|Destination|AverageSpeed|\n",
      "+-----------+------------+\n",
      "|BED        |170.0       |\n",
      "|SYZ        |250.0       |\n",
      "|ICN        |234.2       |\n",
      "|MIA        |170.0       |\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_speed_per_airport_dest = df_absd.groupBy(\"Destination\").agg(F.avg(\"Speed\").alias(\"AverageSpeed\"))\n",
    "avg_speed_per_airport_dest.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Type|AverageSpeed      |\n",
      "+----+------------------+\n",
      "|A320|241.22222222222223|\n",
      "|B738|170.0             |\n",
      "|E545|170.0             |\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_speed_per_aircraft = df_absd.groupBy(\"Type\").agg(F.avg(\"Speed\").alias(\"AverageSpeed\"))\n",
    "avg_speed_per_aircraft.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"Flight\").orderBy(F.desc(\"LastUpdate\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_rn=df_absd.withColumn(\"row_number\",F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|Flight|Speed|LastUpdate|\n",
      "+------+-----+----------+\n",
      "|AAL476|    0|1696350960|\n",
      "| BA484|    0|1696290420|\n",
      "|LXJ476|    0|1696350320|\n",
      "| QR476|    0|1696288335|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_df = df_with_rn.filter(F.col(\"row_number\")==1).select(\"Flight\",\"Speed\",\"LastUpdate\")\n",
    "latest_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|Flight|Speed|LastUpdate|OnGround|\n",
      "+------+-----+----------+--------+\n",
      "|AAL476|    0|1696350960|       1|\n",
      "| BA484|    0|1696290420|       1|\n",
      "|LXJ476|    0|1696350320|       1|\n",
      "| QR476|    0|1696288335|       1|\n",
      "+------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Re doo \n",
    "better_df = df_with_rn.filter(F.col(\"row_number\")==1)\n",
    "better_df = better_df.withColumn(\"Speed\",F.when(F.col(\"OnGround\") == False, 1).otherwise(F.col(\"Speed\"))).select(\"Flight\",\"Speed\",\"LastUpdate\",\"OnGround\")\n",
    "better_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsb_multi.json data successfully loaded\n"
     ]
    }
   ],
   "source": [
    "url_adsb_multi = \"https://raw.githubusercontent.com/JM-AE/Airport-Flight-Data/refs/heads/main/adsb_multi_aircraft.json\"\n",
    "response_adsb_multi = requests.get(url_adsb_multi)\n",
    "\n",
    "if response_adsb_multi.status_code == 200:\n",
    "    data_adsb_multi = response_adsb_multi.json()\n",
    "    print(\"adsb_multi.json data successfully loaded\")\n",
    "else: print(f\"Failed to load adsb_multi.json Status code:{response_adsb_multi.status_code}\")\n",
    "\n",
    "urllib.request.urlretrieve(url_adsb_multi,\"/tmp/file_absd_multi.json\")\n",
    "df_absd_multi =spark.read.json(\"/tmp/file_absd_multi.json\")\n",
    "\n",
    "for entry_adsb_multi in data_adsb_multi[:3]:\n",
    "    print(\"data_adsb_multi\",entry_adsb_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
