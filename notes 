from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
import json

# Sample data
data = [
    {
        "carrier": {
            "iata": "KE",
            "icao": "KAL"
        },
        "flightNumber": 476,
        "departure": {
            "airport": {
                "iata": "SGN",
                "icao": "VVTS"
            },
            "terminal": "2",
            "date": {
                "local": "2023-10-03",
                "utc": "2023-10-02"
            },
            "time": {
                "local": "00:15",
                "utc": "17:15"
            }
        },
        "arrival": {
            "airport": {
                "iata": "ICN",
                "icao": "RKSI"
            },
            "terminal": "2",
            "date": {
                "local": "2023-10-03",
                "utc": "2023-10-02"
            },
            "time": {
                "local": "07:20",
                "utc": "22:20"
            }
        },
        "elapsedTime": 305,
        "codeshare": {
            "cockpitCrewEmployer": {
                "code": "KE",
                "name": "",
                "number": "4"
            },
            "marketingFlights": [
                {"code": "DL", "serviceNumber": "7926"},
                {"code": "VN", "serviceNumber": "3400"},
                {"code": "VS", "serviceNumber": "5520"}
            ]
        }
    }
    # Add more records as necessary
]

# Initialize Spark
spark = SparkSession.builder.master("local[*]").appName("Flatten JSON").getOrCreate()

# Convert the list of dictionaries into a DataFrame
df = spark.read.json(spark.sparkContext.parallelize([json.dumps(data)]))

# Function to flatten nested structure
def flatten_df(df, prefix=None):
    # Loop over columns, checking for structs (nested fields) or arrays
    for col_name in df.columns:
        col = df[col_name]
        if isinstance(df.schema[col_name].dataType, T.StructType):
            # If it's a Struct, expand it
            df = df.select("*", F.col(f"{col_name}.*"))
            df = df.drop(col_name)
        elif isinstance(df.schema[col_name].dataType, T.ArrayType):
            # If it's an Array, explode it
            df = df.withColumn(col_name, F.explode_outer(col))
    return df

# Call the flatten function
flat_df = flatten_df(df)

# Show the result
flat_df.show(truncate=False)